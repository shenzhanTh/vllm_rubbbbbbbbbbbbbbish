Namespace(backend='vllm', dataset='/work/models/data/ShareGPT_V3_unfiltered_cleaned_split.json', input_len=None, output_len=None, model='/work/models/Meta-Llama-3-8B-GPTQ/', tokenizer='/work/models/Meta-Llama-3-8B-GPTQ/', quantization='gptq', tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=32, seed=0, hf_max_batch_size=None, trust_remote_code=True, max_model_len=None, dtype='float16', enforce_eager=True, kv_cache_dtype='auto', device='cuda')
WARNING 10-03 16:31:46 config.py:193] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.
INFO 10-03 16:31:46 llm_engine.py:87] Initializing an LLM engine with config: model='/work/models/Meta-Llama-3-8B-GPTQ/', tokenizer='/work/models/Meta-Llama-3-8B-GPTQ/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1003 16:31:46.970847 32402 ProcessGroupNCCL.cpp:686] [Rank 0] ProcessGroupNCCL initialization options:NCCL_ASYNC_ERROR_HANDLING: 1, NCCL_DESYNC_DEBUG: 0, NCCL_ENABLE_TIMING: 0, NCCL_BLOCKING_WAIT: 0, TIMEOUT(ms): 1800000, USE_HIGH_PRIORITY_STREAM: 0, TORCH_DISTRIBUTED_DEBUG: OFF, NCCL_DEBUG: OFF, ID=93941404809680
I1003 16:31:47.586807 32402 ProcessGroupNCCL.cpp:1340] NCCL_DEBUG: N/A
I1003 16:31:47.603626 32402 ProcessGroupNCCL.cpp:686] [Rank 0] ProcessGroupNCCL initialization options:NCCL_ASYNC_ERROR_HANDLING: 1, NCCL_DESYNC_DEBUG: 0, NCCL_ENABLE_TIMING: 0, NCCL_BLOCKING_WAIT: 0, TIMEOUT(ms): 1800000, USE_HIGH_PRIORITY_STREAM: 0, TORCH_DISTRIBUTED_DEBUG: OFF, NCCL_DEBUG: OFF, ID=93941731218944
I1003 16:31:47.604012 32402 ProcessGroupNCCL.cpp:686] [Rank 0] ProcessGroupNCCL initialization options:NCCL_ASYNC_ERROR_HANDLING: 1, NCCL_DESYNC_DEBUG: 0, NCCL_ENABLE_TIMING: 0, NCCL_BLOCKING_WAIT: 0, TIMEOUT(ms): 1800000, USE_HIGH_PRIORITY_STREAM: 0, TORCH_DISTRIBUTED_DEBUG: OFF, NCCL_DEBUG: OFF, ID=93941404533680
INFO 10-03 16:32:02 llm_engine.py:357] # GPU blocks: 25305, # CPU blocks: 2048
Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:37, 10.88s/it]Processed prompts:   6%|▋         | 2/32 [00:11<02:31,  5.05s/it]Processed prompts:  12%|█▎        | 4/32 [00:14<01:12,  2.61s/it]Processed prompts:  16%|█▌        | 5/32 [00:16<01:07,  2.49s/it]Processed prompts:  19%|█▉        | 6/32 [00:23<01:41,  3.92s/it]Processed prompts:  22%|██▏       | 7/32 [00:27<01:39,  4.00s/it]Processed prompts:  28%|██▊       | 9/32 [00:33<01:22,  3.60s/it]Processed prompts:  31%|███▏      | 10/32 [00:40<01:32,  4.21s/it]Processed prompts:  34%|███▍      | 11/32 [00:44<01:31,  4.37s/it]Processed prompts:  38%|███▊      | 12/32 [00:51<01:42,  5.12s/it]Processed prompts:  41%|████      | 13/32 [00:59<01:48,  5.69s/it]Processed prompts:  47%|████▋     | 15/32 [00:59<00:54,  3.23s/it]Processed prompts:  50%|█████     | 16/32 [01:00<00:42,  2.67s/it]Processed prompts:  53%|█████▎    | 17/32 [01:02<00:38,  2.57s/it]Processed prompts:  56%|█████▋    | 18/32 [01:11<00:59,  4.22s/it]Processed prompts:  59%|█████▉    | 19/32 [01:17<01:00,  4.68s/it]Processed prompts:  62%|██████▎   | 20/32 [01:17<00:41,  3.47s/it]Processed prompts:  66%|██████▌   | 21/32 [01:26<00:54,  4.91s/it]Processed prompts:  69%|██████▉   | 22/32 [01:27<00:38,  3.88s/it]Processed prompts:  72%|███████▏  | 23/32 [01:28<00:26,  2.94s/it]Processed prompts:  75%|███████▌  | 24/32 [01:33<00:28,  3.54s/it]Processed prompts:  78%|███████▊  | 25/32 [01:43<00:38,  5.43s/it]Processed prompts:  81%|████████▏ | 26/32 [01:46<00:28,  4.79s/it]Processed prompts:  84%|████████▍ | 27/32 [01:47<00:18,  3.75s/it]Processed prompts:  88%|████████▊ | 28/32 [01:50<00:13,  3.47s/it]Processed prompts:  91%|█████████ | 29/32 [01:53<00:09,  3.33s/it]Processed prompts:  94%|█████████▍| 30/32 [01:54<00:05,  2.61s/it]Processed prompts:  97%|█████████▋| 31/32 [01:57<00:02,  2.76s/it]Processed prompts: 100%|██████████| 32/32 [01:59<00:00,  2.68s/it]Processed prompts: 100%|██████████| 32/32 [01:59<00:00,  3.75s/it]
Latency: 119.98 s
All Throughput: 0.27 requests/s, 109.62 tokens/s
Generate Throughput: 56.72 tokens/s
