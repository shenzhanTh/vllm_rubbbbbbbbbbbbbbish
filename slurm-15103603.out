Namespace(backend='vllm', dataset=None, input_len=8, output_len=8, model='/work/models/Meta-Llama-3-8B/', tokenizer='/work/models/Meta-Llama-3-8B/', quantization=None, tensor_parallel_size=1, n=1, use_beam_search=False, num_prompts=8, seed=0, hf_max_batch_size=None, trust_remote_code=True, max_model_len=None, dtype='float16', enforce_eager=True, kv_cache_dtype='auto', device='cuda')
WARNING 10-03 17:07:16 config.py:618] Casting torch.bfloat16 to torch.float16.
INFO 10-03 17:07:16 llm_engine.py:87] Initializing an LLM engine with config: model='/work/models/Meta-Llama-3-8B/', tokenizer='/work/models/Meta-Llama-3-8B/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1003 17:07:16.852715 60701 ProcessGroupNCCL.cpp:686] [Rank 0] ProcessGroupNCCL initialization options:NCCL_ASYNC_ERROR_HANDLING: 1, NCCL_DESYNC_DEBUG: 0, NCCL_ENABLE_TIMING: 0, NCCL_BLOCKING_WAIT: 0, TIMEOUT(ms): 1800000, USE_HIGH_PRIORITY_STREAM: 0, TORCH_DISTRIBUTED_DEBUG: OFF, NCCL_DEBUG: OFF, ID=93849344240224
I1003 17:07:17.712849 60701 ProcessGroupNCCL.cpp:1340] NCCL_DEBUG: N/A
I1003 17:07:17.728657 60701 ProcessGroupNCCL.cpp:686] [Rank 0] ProcessGroupNCCL initialization options:NCCL_ASYNC_ERROR_HANDLING: 1, NCCL_DESYNC_DEBUG: 0, NCCL_ENABLE_TIMING: 0, NCCL_BLOCKING_WAIT: 0, TIMEOUT(ms): 1800000, USE_HIGH_PRIORITY_STREAM: 0, TORCH_DISTRIBUTED_DEBUG: OFF, NCCL_DEBUG: OFF, ID=93849362284256
I1003 17:07:17.728960 60701 ProcessGroupNCCL.cpp:686] [Rank 0] ProcessGroupNCCL initialization options:NCCL_ASYNC_ERROR_HANDLING: 1, NCCL_DESYNC_DEBUG: 0, NCCL_ENABLE_TIMING: 0, NCCL_BLOCKING_WAIT: 0, TIMEOUT(ms): 1800000, USE_HIGH_PRIORITY_STREAM: 0, TORCH_DISTRIBUTED_DEBUG: OFF, NCCL_DEBUG: OFF, ID=93849361632144
INFO 10-03 17:07:35 llm_engine.py:357] # GPU blocks: 20469, # CPU blocks: 2048
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s]Processed prompts:  12%|█▎        | 1/8 [00:00<00:02,  3.09it/s]Processed prompts: 100%|██████████| 8/8 [00:00<00:00, 24.73it/s]
Latency: 0.33 s
All Throughput: 24.43 requests/s, 390.95 tokens/s
Generate Throughput: 195.48 tokens/s
